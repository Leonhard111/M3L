diff --git a/models/ppo_dino.py b/models/ppo_dino.py
index b60efcf..0d9ef97 100644
--- a/models/ppo_dino.py
+++ b/models/ppo_dino.py
@@ -185,39 +185,39 @@ class PPO_DINO(OnPolicyAlgorithm):
             # 估算总epoch数(允许用户训练更长时间)
             num_epochs = 1000  # 使用一个足够大的值，让VTDINO的scheduler能够正常工作
             
-            # 调用VTDINO的configure_optimizers方法获取优化器和调度器
-            if hasattr(self.dino, 'configure_optimizers'):
-                try:
-                    dino_optimizer, lr_scheduler_dict, wd_scheduler_dict = self.dino.configure_optimizers(
-                        num_iterations_per_epoch=num_iterations_per_epoch,
-                        num_epochs=num_epochs
-                    )
-                    self.dino_optimizer = dino_optimizer
+            # # 调用VTDINO的configure_optimizers方法获取优化器和调度器
+            # if hasattr(self.dino, 'configure_optimizers'):
+            #     try:
+            #         dino_optimizer, lr_scheduler_dict, wd_scheduler_dict = self.dino.configure_optimizers(
+            #             num_iterations_per_epoch=num_iterations_per_epoch,
+            #             num_epochs=num_epochs
+            #         )
+            #         self.dino_optimizer = dino_optimizer
                     
-                    # 保存调度器以便后续使用
-                    if lr_scheduler_dict is not None:
-                        self.dino_lr_scheduler = lr_scheduler_dict.get('scheduler')
-                    else:
-                        self.dino_lr_scheduler = None
+            #         # 保存调度器以便后续使用
+            #         if lr_scheduler_dict is not None:
+            #             self.dino_lr_scheduler = lr_scheduler_dict.get('scheduler')
+            #         else:
+            #             self.dino_lr_scheduler = None
                         
-                    if wd_scheduler_dict is not None:
-                        self.dino_wd_scheduler = wd_scheduler_dict.get('wd_scheduler')
-                    else:
-                        self.dino_wd_scheduler = None
+            #         if wd_scheduler_dict is not None:
+            #             self.dino_wd_scheduler = wd_scheduler_dict.get('wd_scheduler')
+            #         else:
+            #             self.dino_wd_scheduler = None
                         
-                    if self.verbose > 0:
-                        print(f"Successfully configured DINO optimizer and scheduler from VTDINO class")
-                except Exception as e:
-                    # 如果配置失败，回退到简单的优化器
-                    print(f"Error configuring DINO optimizer from class: {e}. Using default optimizer.")
-                    self.dino_optimizer = th.optim.Adam(self.dino.parameters(), lr=1e-4)
-                    self.dino_lr_scheduler = None
-                    self.dino_wd_scheduler = None
-            else:
-                # 如果DINO没有configure_optimizers方法，使用默认优化器
-                self.dino_optimizer = th.optim.Adam(self.dino.parameters(), lr=1e-4)
-                self.dino_lr_scheduler = None
-                self.dino_wd_scheduler = None
+            #         if self.verbose > 0:
+            #             print(f"Successfully configured DINO optimizer and scheduler from VTDINO class")
+            #     except Exception as e:
+            #         # 如果配置失败，回退到简单的优化器
+            #         print(f"Error configuring DINO optimizer from class: {e}. Using default optimizer.")
+            #         self.dino_optimizer = th.optim.Adam(self.dino.parameters(), lr=1e-4)
+            #         self.dino_lr_scheduler = None
+            #         self.dino_wd_scheduler = None
+            # else:
+            #     # 如果DINO没有configure_optimizers方法，使用默认优化器
+            #     self.dino_optimizer = th.optim.Adam(self.dino.parameters(), lr=1e-4)
+            #     self.dino_lr_scheduler = None
+            #     self.dino_wd_scheduler = None
 
     def _setup_model(self) -> None:
         super()._setup_model()
@@ -318,7 +318,7 @@ class PPO_DINO(OnPolicyAlgorithm):
                     observations['tactile'] = observations['tactile'].reshape((observations['tactile'].shape[0], -1, observations['tactile'].shape[3], observations['tactile'].shape[4]))
                 
                 # 清零DINO的优化器梯度
-                self.dino_optimizer.zero_grad()
+                #self.dino_optimizer.zero_grad()
                 # 清零策略的优化器梯度
                 self.policy.optimizer.zero_grad()
                 
@@ -417,7 +417,7 @@ class PPO_DINO(OnPolicyAlgorithm):
                 break
             
             # 在每个epoch结束时调用DINO的回调方法
-            self.dino.on_train_epoch_end()
+#            self.dino.on_train_epoch_end()
 
         explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())
 
diff --git a/models/pretrain_policy.py b/models/pretrain_policy.py
index d1a9650..db81fe9 100644
--- a/models/pretrain_policy.py
+++ b/models/pretrain_policy.py
@@ -46,7 +46,7 @@ class DINOExtractor(BaseFeaturesExtractor):
 
     def __init__(self, 
                     observation_space: gym.Space, 
-                    dino_model, 
+                    dino_model,    # 换成encoder
                     dim_embeddings, 
                     vision_only_control, 
                     frame_stack
diff --git a/trainDINO.py b/trainDINO.py
index 7a9476a..b8eeb5d 100644
--- a/trainDINO.py
+++ b/trainDINO.py
@@ -48,7 +48,7 @@ def main():
             "HandManipulatePenRotateFixed-v1"
         ],
     )
-    parser.add_argument("--n_envs", type=int, default=8)
+    parser.add_argument("--n_envs", type=int, default=1)
     parser.add_argument(
         "--state_type",
         type=str,
@@ -59,12 +59,12 @@ def main():
     parser.add_argument("--use_latch", type=str2bool, default=True)
     
     parser.add_argument("--camera_idx", type=int, default=0, choices=[0, 1, 2, 3])
-    parser.add_argument("--frame_stack", type=int, default=4)
+    parser.add_argument("--frame_stack", type=int, default=1)
     parser.add_argument("--no_rotation", type=str2bool, default=True)
 
     # DINO参数
     parser.add_argument("--representation", type=str2bool, default=True)
-    parser.add_argument("--dim_embedding", type=int, default=256)
+    parser.add_argument("--dim_embedding", type=int, default=384)             # encoder输出维度
     parser.add_argument("--use_sincosmod_encodings", type=str2bool, default=True)
     parser.add_argument("--num_global_masks", type=int, default=2)
     parser.add_argument("--num_local_masks", type=int, default=8)
@@ -78,15 +78,15 @@ def main():
     parser.add_argument("--teacher_temp_max", type=float, default=0.07)
     parser.add_argument("--teacher_warmup_epochs", type=int, default=10)
     
-    parser.add_argument("--dino_batch_size", type=int, default=512)
+    parser.add_argument("--dino_batch_size", type=int, default=128)
     parser.add_argument("--train_dino_every", type=int, default=1)
 
     # PPO参数
-    parser.add_argument("--rollout_length", type=int, default=32768)
+    parser.add_argument("--rollout_length", type=int, default=512)             #32768
     parser.add_argument("--ppo_epochs", type=int, default=10)
     parser.add_argument("--lr_ppo", type=float, default=1e-4)
     parser.add_argument("--vision_only_control", type=str2bool, default=False)
-    parser.add_argument("--batch_size", type=int, default=512)
+    parser.add_argument("--batch_size", type=int, default=128)
 
     # PPO-DINO参数
     parser.add_argument("--separate_optimizer", type=str2bool, default=True)
@@ -138,22 +138,22 @@ def main():
     env = VecNormalize(env, norm_obs=False, norm_reward=config.norm_reward)
     # encoder = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg')
     # 创建VTT实例作为encoder
-    encoder = VTT(
-        image_size=(64, 64),
-        tactile_size=(32, 32),
-        image_patch_size=8,
-        tactile_patch_size=4,
-        dim=config.dim_embedding,
-        depth=4,
-        heads=8,
-        mlp_dim=config.dim_embedding * 2,
-        num_tactiles=num_tactiles,
-        image_channels=3*config.frame_stack,
-        tactile_channels=3*config.frame_stack,
-        frame_stack=config.frame_stack,
-        num_register_tokens=1,
-        pos_embed_fn="sinusoidal" if config.use_sincosmod_encodings else "learned"
-    )
+    # encoder = VTT(
+    #     image_size=(64, 64),
+    #     tactile_size=(32, 32),
+    #     image_patch_size=8,
+    #     tactile_patch_size=4,
+    #     dim=config.dim_embedding,
+    #     depth=4,
+    #     heads=8,
+    #     mlp_dim=config.dim_embedding * 2,
+    #     num_tactiles=num_tactiles,
+    #     image_channels=3*config.frame_stack,
+    #     tactile_channels=3*config.frame_stack,
+    #     frame_stack=config.frame_stack,
+    #     num_register_tokens=1,
+    #     pos_embed_fn="sinusoidal" if config.use_sincosmod_encodings else "learned"
+    # )
 
 
     encoder = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg')
@@ -161,47 +161,47 @@ def main():
         param.requires_grad = False
         
     # 创建DINOHead实例 - 调整输出维度
-    dino_head_partial = partial(
-        DINOHead, 
-        out_dim=8192,  # 修改为8192，更适合256维度的token  8192 = 256 * 32
-        use_bn=False, 
-        nlayers=3, 
-        hidden_dim=2048, 
-        bottleneck_dim=256
-    )
+    # dino_head_partial = partial(
+    #     DINOHead, 
+    #     out_dim=8192,  # 修改为8192，更适合256维度的token  8192 = 256 * 32
+    #     use_bn=False, 
+    #     nlayers=3, 
+    #     hidden_dim=2048, 
+    #     bottleneck_dim=256
+    # )
     
-    # 创建优化器配置
-    optim_cfg = partial(torch.optim.AdamW, lr=1e-4, weight_decay=0.05)
-    lr_scheduler_cfg = partial(torch.optim.lr_scheduler.CosineAnnealingLR, eta_min=1e-6)
+    # # 创建优化器配置
+    # optim_cfg = partial(torch.optim.AdamW, lr=1e-4, weight_decay=0.05)
+    # lr_scheduler_cfg = partial(torch.optim.lr_scheduler.CosineAnnealingLR, eta_min=1e-6)
     
-    # 创建VTDINO实例
-    dino = VTDINO(
-        encoder=encoder,
-        dino_head=dino_head_partial,
-        optim_cfg=optim_cfg,
-        lr_scheduler_cfg=lr_scheduler_cfg,
-        wd_scheduler_cfg=None,
-        local_mask_scale=(config.local_mask_scale_min, config.local_mask_scale_max),
-        global_mask_scale=(config.global_mask_scale_min, config.global_mask_scale_max),
-        num_global_masks=config.num_global_masks, 
-        num_local_masks=config.num_local_masks,
-        min_keep_num_sensors=4,
-        allow_mask_overlap=config.allow_mask_overlap,
-        moving_average_decay=config.moving_average_decay,
-        teacher_temp=[config.teacher_temp_min, config.teacher_temp_max],
-        teacher_warmup_epochs=config.teacher_warmup_epochs,
-        use_momentum=True,
-    )
+    # # 创建VTDINO实例
+    # dino = VTDINO(
+    #     encoder=encoder,
+    #     dino_head=dino_head_partial,
+    #     optim_cfg=optim_cfg,
+    #     lr_scheduler_cfg=lr_scheduler_cfg,
+    #     wd_scheduler_cfg=None,
+    #     local_mask_scale=(config.local_mask_scale_min, config.local_mask_scale_max),
+    #     global_mask_scale=(config.global_mask_scale_min, config.global_mask_scale_max),
+    #     num_global_masks=config.num_global_masks, 
+    #     num_local_masks=config.num_local_masks,
+    #     min_keep_num_sensors=4,
+    #     allow_mask_overlap=config.allow_mask_overlap,
+    #     moving_average_decay=config.moving_average_decay,
+    #     teacher_temp=[config.teacher_temp_min, config.teacher_temp_max],
+    #     teacher_warmup_epochs=config.teacher_warmup_epochs,
+    #     use_momentum=True,
+    # )
     
-    if torch.cuda.is_available():
-        dino.cuda()
-    dino.current_teacher_temp = config.teacher_temp_min
-    dino.eval()
+    # if torch.cuda.is_available():
+    #     dino.cuda()
+    # dino.current_teacher_temp = config.teacher_temp_min
+    # dino.eval()
 
     if config.representation:
         policy = DINOPolicy
         policy_kwargs={
-            "dino_model": dino,
+            "dino_model": encoder,
             "dim_embeddings": config.dim_embedding,
             "vision_only_control": config.vision_only_control,
             "net_arch": dict(pi=[256, 256], vf=[256, 256]),
@@ -222,7 +222,7 @@ def main():
             dino_batch_size=config.dino_batch_size,
             separate_optimizer=config.separate_optimizer,
             policy_kwargs=policy_kwargs,
-            dino=dino,  # 直接传递dino实例，PPO_DINO会正确配置其优化器
+            dino=encoder,  # 直接传递dino实例，PPO_DINO会正确配置其优化器
         )
         
         callbacks = create_callbacks(
@@ -243,13 +243,13 @@ def main():
             n_steps=config.rollout_length // config.n_envs,
             n_epochs=config.ppo_epochs,
             policy_kwargs={
-                "dino_model": dino,
+                "dino_model": encoder,
                 "dim_embeddings": config.dim_embedding,
                 "vision_only_control": config.vision_only_control,
                 "net_arch": dict(pi=[256, 256], vf=[256, 256]),
                 "frame_stack": config.frame_stack,
             },
-            dino=dino,
+            dino=encoder,
             dino_batch_size=config.dino_batch_size,
         )
         callbacks = create_callbacks(
